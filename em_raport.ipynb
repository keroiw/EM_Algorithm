{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metody klasyfikacji i redukcji wymiaru\n",
    "\n",
    "### Zastosowanie algorytmu EM do uproszczonej wersji znajdowania motywów w ciągach DNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Błażej Wiórek, Zofia Dziedzic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opis problemu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obserwujemy wektor losowy $\\mathbf{X} = (X_1, X_2, \\ldots, X_w)$, taki że $X_1, X_2, \\ldots, X_w$ są niezależne o rozkładzie\n",
    "\n",
    "$$\n",
    "    p(x_i; \\mathbf{\\theta_i}) = \\theta_{1i}\\mathbb{1}_{\\{1\\}}(x_i) + \\theta_{2i}\\mathbb{1}_{\\{2\\}}(x_i) + \\theta_{3i}\\mathbb{1}_{\\{3\\}}(x_i) + \\theta_{4i}\\mathbb{1}_{\\{4\\}}(x_i),\n",
    "$$\n",
    "\n",
    "gdzie $\\mathbf{\\theta_i} = (\\theta_{1i}, \\theta_{2i}, \\theta_{3i}, \\theta_{4i})^T$. Oznacza to, że zmienne $X_i$, $i \\in \\{1,2,\\ldots,w\\}$ przyjmują wartości ze zbioru $\\{1,2,3,4\\}$ odpowiednio z prawdopodobieństwami $\\theta_{1i}, \\theta_{2i}, \\theta_{3i}, \\theta_{4i}$. Wobec tego\n",
    "\n",
    "$$\n",
    "    p(\\mathbf{x}; \\mathbf{\\theta}) = \\prod_{i=1}^{w} p(x_i; \\mathbf{\\theta_i}).\n",
    "$$\n",
    "\n",
    "Naszym zadaniem będzie modelowanie tego wektora w sytuacji, gdy $\\mathbf{\\theta_i}, i = 1, 2, \\ldots, w$ może przyjmować jedną z dwóch postaci\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{\\theta^{(0)}_i} = (\\theta^{(0)}_{1i}, \\theta^{(0)}_{2i}, \\theta^{(0)}_{3i}, \\theta^{(0)}_{4i})^T,\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{\\theta^{(1)}_i} = (\\theta^{(1)}_{1i}, \\theta^{(1)}_{2i}, \\theta^{(1)}_{3i}, \\theta^{(1)}_{4i})^T.\n",
    "$$\n",
    "\n",
    "Oznaczmy przez $\\mathbf{\\Theta^{(0)}}$ macierz, której elementami są $\\theta^{(0)}_{ki}$, $k = 1,2,3,4$ i, analogicznie, $\\mathbf{\\Theta^{(1)}} = (\\mathbf{\\theta^{(1)}_1, \\theta^{(1)}_2, \\ldots, \\theta^{(1)}_w})$.\n",
    "O tym, z którego z rozkładów - $p(\\mathbf{x}; \\mathbf{\\theta^{(0)}})$ czy $p(\\mathbf{x}; \\mathbf{\\theta^{(1)}})$ - pochodzi $X$, decyduje inna zmienna losowa, $Z$, która z p-stwem $\\alpha_0$ jest równa $0$ i z p-stwem $\\alpha_1 = 1 - \\alpha_0$ jest równa $1$.\n",
    "Niech\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_1 = x_{11}, x_{12}, \\ldots, x_{1i}, \\ldots, x_{1w} \\\\\n",
    "\\mathbf{x}_2 = x_{21}, x_{22}, \\ldots, x_{2i}, \\ldots, x_{2w} \\\\\n",
    "\\ldots \\\\\n",
    "\\mathbf{x}_j = x_{j1}, x_{j2}, \\ldots, x_{ji}, \\ldots, x_{jw} \\\\\n",
    "\\ldots \\\\\n",
    "\\mathbf{x}_k = x_{k1}, x_{k2}, \\ldots, x_{ki}, \\ldots, x_{kw}\n",
    "$$\n",
    "\n",
    "będą zaobserwowanymi niezależnymi realizacjami wektora losowego $\\mathbf{X}$. Każdej z nich odpowiada $z_j$, $j = 1, \\ldots, k$, które decyduje o rozkładzie $X_j$.\n",
    "\n",
    "W oparciu o daną próbę, ale nie znając wartości $z_1, \\ldots, z_k$, chcemy wyestymować parametry $\\mathbf{\\Theta} = (\\mathbf{\\Theta^{(0)}}, \\mathbf{\\Theta^{(1)}})$ i $\\alpha = (\\alpha_0, \\alpha_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funkcja wiarogodności"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametrów $\\hat{\\mathbf{\\Theta}}$ i $\\hat\\alpha$ będziemy szukać, maksymalizując funckcję wiarygodności\n",
    "$$\n",
    "l(\\mathbf{\\Theta},\\alpha) = \\log L(\\mathbf{\\Theta}, \\alpha) = \\sum_{j=1}^k \\log p(\\mathbf{x_j}, z_j; \\mathbf{\\Theta}, \\alpha) = \n",
    " \\sum_{j=1}^k \\log (p(\\mathbf{x_j}, \\mathbf{\\Theta^{(0)}})\\alpha_0 + p(\\mathbf{x_j}, \\mathbf{\\Theta^{(1)}})\\alpha_1).\n",
    "$$\n",
    "\n",
    "Ponieważ nie jesteśmy w stanie znaleźć estymatorów największej wiarygodności analitycznie, użyjemy w tym celu algorytmu EM, który opisujemy krótko poniżej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorytm EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea tej procedury opiera się na fakcie, że dla znanego $\\mathbf{z} = (z_1, z_2, \\ldots, z_k)^T$ łatwo znaleźć $\\hat{\\mathbf{\\Theta}}$ i $\\alpha$. Algorytm składa się więc z następujących dwóch kroków, które po odpowiedniej liczbie iteracji pozwalają skutecznie wyestymować szukane parametry:\n",
    "\n",
    "   1) **Expectation**\n",
    "    \n",
    "   \"zgadnięcie\" $\\mathbf{z}$, które w praktyce polega na obliczeniu \"wag\" $\\mathbf{w_0} = (w_0^{(1)},\\ldots,w_0^{(k)})$, \n",
    "   $\\mathbf{w_1} = (w_1^{(1)},\\ldots,w_1^{(k)})$, takich że\n",
    "   \n",
    "   $$\n",
    "   w_0^{(j)} = \\mathbb{P}(z_j = 0|\\mathbf{X_j} = \\mathbf{x_j}; \\mathbf{\\Theta}) = \n",
    "   \\frac{\\mathbb{P}(\\mathbf{X_j} = \\mathbf{x_j}|z_j = 0; \\mathbf{\\Theta^{(0)})}\\alpha_0}{\\mathbb{P}(\\mathbf{X_j} = \\mathbf{x_j}|z_j = 0; \\mathbf{\\Theta^{(0)})}\\alpha_0 + \\mathbb{P}(\\mathbf{X_j} = \n",
    "   \\mathbf{x_j}|z_j = 1; \\mathbf{\\Theta^{(1)})}\\alpha_1} =\n",
    "   \\frac{\\prod_{i=1}^{w}\\theta^{(0)}_{x_{ji}}\\alpha_0}{\\prod_{i=1}^{w}\\theta^{(0)}_{x_{ji}}\\alpha_0 + \\prod_{i=1}^{w}\\theta^{(1)}_{x_{ji}}\\alpha_1}\n",
    "   $$\n",
    "   i\n",
    "    $$\n",
    "   w_1^{(j)} = \\mathbb{P}(z_j = 1|\\mathbf{X_j} = \\mathbf{x_j}; \\mathbf{\\Theta}) = \n",
    "   \\frac{\\mathbb{P}(\\mathbf{X_j} = \\mathbf{x_j}|z_j = 1; \\mathbf{\\Theta^{(1)})}\\alpha_1}\n",
    "   {\\mathbb{P}(\\mathbf{X_j} = \\mathbf{x_j}|z_j = 0; \\mathbf{\\Theta^{(0)}})\\alpha_0 \n",
    "   + \\mathbb{P}(\\mathbf{X_j} = \\mathbf{x_j}|z_j = 1; \\mathbf{\\Theta^{(1)}})\\alpha_1} = \n",
    "   \\frac{\\prod_{i=1}^{w}\\theta^{(1)}_{x_{ji}}\\alpha_1}{\\prod_{i=1}^{w}\\theta^{(0)}_{x_{ji}}\\alpha_0 +\n",
    "   \\prod_{i=1}^{w}\\theta^{(1)}_{x_{ji}}\\alpha_1}.\n",
    "   $$\n",
    "   \n",
    "   2) **Maximization** ?To chyba nie jest ok?\n",
    "   \n",
    "   Dla danych $\\mathbf{w_0}$ i $\\mathbf{w_1}$ wyliczymy \n",
    "   \n",
    "   $$ \n",
    "   \\hat{\\Theta^{(0)}} = \\frac{\\sum_{j=1}^k w_0^{(j)}\\mathbf{x_j}}{\\sum_{j=1}^k w_0^{(j)}},\n",
    "   $$\n",
    "   \n",
    "   $$ \n",
    "   \\hat{\\Theta^{(1)}} = \\frac{\\sum_{j=1}^k w_1^{(j)}\\mathbf{x_j}}{\\sum_{j=1}^k w_1^{(j)}},\n",
    "   $$\n",
    "   \n",
    "   $$\n",
    "   \\alpha???.\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jakieś wykresiki, jak dobrze EM przybliżył thetę? Plus normy ||theta - theta_est||_2?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
